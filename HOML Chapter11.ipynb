{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On Machine Learning CH.11 심층 신경망 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return\"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_hidden = 100\n",
    "n_inputs = 28*28\n",
    "n_outputs = 5\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "hidden_layer_100 = partial(tf.layers.dense, units=n_hidden, kernel_initializer=he_init, activation=tf.nn.elu)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"network\"):\n",
    "    hidden1 = hidden_layer_100(X, name=\"hidden1\")\n",
    "    hidden2 = hidden_layer_100(hidden1, name=\"hidden2\")\n",
    "    hidden3 = hidden_layer_100(hidden2, name=\"hidden3\")\n",
    "    hidden4 = hidden_layer_100(hidden3, name=\"hidden4\")\n",
    "    hidden5 = hidden_layer_100(hidden4, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar(\"log_loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"training\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"CH11\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.96 Validation accuracy: 0.98279905 Loss: 0.050905496\n",
      "5 Train accuracy: 1.0 Validation accuracy: 0.9843628 Loss: 0.047521077\n",
      "10 Train accuracy: 1.0 Validation accuracy: 0.9937451 Loss: 0.025883293\n",
      "15 Train accuracy: 1.0 Validation accuracy: 0.99296325 Loss: 0.02951165\n",
      "20 Train accuracy: 1.0 Validation accuracy: 0.9921814 Loss: 0.039870024\n",
      "25 Train accuracy: 1.0 Validation accuracy: 0.9913995 Loss: 0.043114018\n",
      "30 Train accuracy: 1.0 Validation accuracy: 0.98944485 Loss: 0.04983349\n",
      "35 Train accuracy: 1.0 Validation accuracy: 0.994527 Loss: 0.038883425\n",
      "40 Train accuracy: 1.0 Validation accuracy: 0.99296325 Loss: 0.031353764\n",
      "45 Train accuracy: 1.0 Validation accuracy: 0.994527 Loss: 0.03259209\n",
      "50 Train accuracy: 1.0 Validation accuracy: 0.9953088 Loss: 0.036789335\n",
      "55 Train accuracy: 1.0 Validation accuracy: 0.9953088 Loss: 0.040813964\n",
      "60 Train accuracy: 1.0 Validation accuracy: 0.9953088 Loss: 0.04507263\n",
      "65 Train accuracy: 1.0 Validation accuracy: 0.9953088 Loss: 0.049542584\n",
      "학습 종료\n",
      "INFO:tensorflow:Restoring parameters from models/11. 8_dnn_model\n",
      "최종 정확도: 0.99494064\n"
     ]
    }
   ],
   "source": [
    "#Run\n",
    "\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "\n",
    "max_epochs_without_progress = 50\n",
    "epochs_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "checkpoint_path = \"tmp/11. 8_dnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"models/11. 8_dnn_model\"\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, 'rb') as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"중지되었던 훈련입니다. 이어서 훈련합니다.\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        init.run()\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_idices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_idices], y_train1[rnd_idices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val, loss_val, summary_str = sess.run([accuracy, loss, loss_summary], feed_dict={X:X_valid1, y: y_valid1})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Validation accuracy:\", acc_val, \"Loss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch+1))\n",
    "            if loss_val < best_loss:\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"학습 종료\")\n",
    "                    break\n",
    "    \n",
    "    save_path = saver.save(sess, final_model_path)\n",
    "    os.remove(checkpoint_epoch_path)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"최종 정확도:\", acc_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"models/11. 8_dnn_model.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss/loss:0\")\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"network/logits/BiasAdd:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "loss_summary = tf.summary.scalar(\"log_loss\", loss)\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"CH11_9_1\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2_full = X_train[y_train >= 5]\n",
    "y_train2_full = y_train[y_train >= 5] - 5\n",
    "X_valid2_full = X_valid[y_valid >= 5]\n",
    "y_valid2_full = y_valid[y_valid >= 5] - 5\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.66 Validation accuracy: 0.7 Loss: 0.9339136\n",
      "5 Train accuracy: 0.88 Validation accuracy: 0.8066667 Loss: 0.57316726\n",
      "10 Train accuracy: 0.84 Validation accuracy: 0.84 Loss: 0.5393415\n",
      "15 Train accuracy: 0.94 Validation accuracy: 0.8466667 Loss: 0.5369514\n",
      "20 Train accuracy: 0.9 Validation accuracy: 0.8333333 Loss: 0.5480967\n",
      "25 Train accuracy: 0.94 Validation accuracy: 0.82 Loss: 0.55014515\n",
      "30 Train accuracy: 0.92 Validation accuracy: 0.8333333 Loss: 0.5620701\n",
      "35 Train accuracy: 1.0 Validation accuracy: 0.82 Loss: 0.5773047\n",
      "40 Train accuracy: 0.92 Validation accuracy: 0.84 Loss: 0.5906904\n",
      "45 Train accuracy: 0.9 Validation accuracy: 0.8333333 Loss: 0.60866106\n",
      "50 Train accuracy: 0.96 Validation accuracy: 0.8333333 Loss: 0.61635536\n",
      "55 Train accuracy: 0.96 Validation accuracy: 0.8333333 Loss: 0.6307665\n",
      "60 Train accuracy: 0.96 Validation accuracy: 0.8333333 Loss: 0.6514885\n",
      "65 Train accuracy: 0.98 Validation accuracy: 0.82666665 Loss: 0.6687559\n",
      "70 Train accuracy: 0.98 Validation accuracy: 0.82666665 Loss: 0.66874367\n",
      "학습 종료\n",
      "INFO:tensorflow:Restoring parameters from models/11. 9_1_dnn_model\n",
      "최종 정확도: 0.8167044\n"
     ]
    }
   ],
   "source": [
    "#Run\n",
    "\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "\n",
    "max_epochs_without_progress = 50\n",
    "epochs_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "checkpoint_path = \"tmp/11. 9_1_dnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"models/11. 9_1_dnn_model\"\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, 'rb') as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"중지되었던 훈련입니다. 이어서 훈련합니다.\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        init.run()\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_idices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_idices], y_train2[rnd_idices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val, loss_val, summary_str = sess.run([accuracy, loss, loss_summary], feed_dict={X:X_valid2, y: y_valid2})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Validation accuracy:\", acc_val, \"Loss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch+1))\n",
    "            if loss_val < best_loss:\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"학습 종료\")\n",
    "                    break\n",
    "    \n",
    "    save_path = saver.save(sess, final_model_path)\n",
    "    os.remove(checkpoint_epoch_path)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"최종 정확도:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"network/hidden5/Elu:0\")\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"CH11_9_2\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.62 Validation accuracy: 0.64 Loss: 1.0881445\n",
      "5 Train accuracy: 0.68 Validation accuracy: 0.7866667 Loss: 0.67234427\n",
      "10 Train accuracy: 0.84 Validation accuracy: 0.81333333 Loss: 0.58038753\n",
      "15 Train accuracy: 0.84 Validation accuracy: 0.8333333 Loss: 0.5531565\n",
      "20 Train accuracy: 0.84 Validation accuracy: 0.8066667 Loss: 0.54371\n",
      "25 Train accuracy: 0.84 Validation accuracy: 0.82666665 Loss: 0.5447029\n",
      "30 Train accuracy: 0.92 Validation accuracy: 0.8466667 Loss: 0.5239546\n",
      "35 Train accuracy: 0.94 Validation accuracy: 0.85333335 Loss: 0.52610517\n",
      "40 Train accuracy: 0.9 Validation accuracy: 0.85333335 Loss: 0.5235795\n",
      "45 Train accuracy: 0.9 Validation accuracy: 0.86 Loss: 0.524925\n",
      "50 Train accuracy: 0.96 Validation accuracy: 0.85333335 Loss: 0.5191049\n",
      "55 Train accuracy: 0.94 Validation accuracy: 0.8466667 Loss: 0.51292664\n",
      "60 Train accuracy: 0.94 Validation accuracy: 0.85333335 Loss: 0.524387\n",
      "65 Train accuracy: 1.0 Validation accuracy: 0.85333335 Loss: 0.53161776\n",
      "70 Train accuracy: 0.9 Validation accuracy: 0.87333333 Loss: 0.5201619\n",
      "75 Train accuracy: 0.92 Validation accuracy: 0.8666667 Loss: 0.5270449\n",
      "80 Train accuracy: 0.94 Validation accuracy: 0.86 Loss: 0.5250595\n",
      "85 Train accuracy: 0.92 Validation accuracy: 0.85333335 Loss: 0.5295758\n",
      "90 Train accuracy: 0.92 Validation accuracy: 0.8466667 Loss: 0.5347273\n",
      "95 Train accuracy: 1.0 Validation accuracy: 0.86 Loss: 0.5421404\n",
      "학습 종료\n",
      "INFO:tensorflow:Restoring parameters from models/11. 9_2_dnn_model\n",
      "최종 정확도: 0.80703557\n"
     ]
    }
   ],
   "source": [
    "#Run\n",
    "\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "\n",
    "max_epochs_without_progress = 50\n",
    "epochs_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "checkpoint_path = \"tmp/11. 9_2_dnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"models/11. 9_2_dnn_model\"\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, 'rb') as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"중지되었던 훈련입니다. 이어서 훈련합니다.\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        init.run()\n",
    "        \n",
    "    hidden_5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden_5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_idices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h5_batch, y_batch = hidden_5_train[rnd_idices], y_train2[rnd_idices]\n",
    "            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={hidden5_out: h5_batch, y: y_batch})\n",
    "        acc_val, loss_val, summary_str = sess.run([accuracy, loss, loss_summary], feed_dict={hidden5_out: hidden_5_valid, y: y_valid2})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Validation accuracy:\", acc_val, \"Loss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch+1))\n",
    "            if loss_val < best_loss:\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"학습 종료\")\n",
    "                    break\n",
    "    \n",
    "    save_path = saver.save(sess, final_model_path)\n",
    "    os.remove(checkpoint_epoch_path)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"최종 정확도:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"models/11. 8_dnn_model.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "hidden4_out = tf.get_default_graph().get_tensor_by_name(\"network/hidden4/Elu:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "saver = tf.train.Saver()\n",
    "loss_summary = tf.summary.scalar(\"log_loss\", loss)\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"CH11_9_3\"), tf.get_default_graph())\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.652 Validation accuracy: 0.6533333 Loss: 0.9893698\n",
      "5 Train accuracy: 0.848 Validation accuracy: 0.8 Loss: 0.619732\n",
      "10 Train accuracy: 0.884 Validation accuracy: 0.84 Loss: 0.5735337\n",
      "15 Train accuracy: 0.904 Validation accuracy: 0.85333335 Loss: 0.5475281\n",
      "20 Train accuracy: 0.904 Validation accuracy: 0.8466667 Loss: 0.52696115\n",
      "25 Train accuracy: 0.926 Validation accuracy: 0.86 Loss: 0.5242251\n",
      "30 Train accuracy: 0.934 Validation accuracy: 0.86 Loss: 0.51448816\n",
      "35 Train accuracy: 0.942 Validation accuracy: 0.86 Loss: 0.5210268\n",
      "40 Train accuracy: 0.948 Validation accuracy: 0.86 Loss: 0.50430024\n",
      "45 Train accuracy: 0.952 Validation accuracy: 0.8666667 Loss: 0.5016423\n",
      "50 Train accuracy: 0.96 Validation accuracy: 0.86 Loss: 0.5115117\n",
      "55 Train accuracy: 0.962 Validation accuracy: 0.85333335 Loss: 0.50982547\n",
      "60 Train accuracy: 0.956 Validation accuracy: 0.8666667 Loss: 0.5223581\n",
      "65 Train accuracy: 0.962 Validation accuracy: 0.86 Loss: 0.51177615\n",
      "70 Train accuracy: 0.966 Validation accuracy: 0.8666667 Loss: 0.51023203\n",
      "75 Train accuracy: 0.964 Validation accuracy: 0.86 Loss: 0.52502817\n",
      "80 Train accuracy: 0.972 Validation accuracy: 0.8666667 Loss: 0.520219\n",
      "85 Train accuracy: 0.972 Validation accuracy: 0.8666667 Loss: 0.5225172\n",
      "90 Train accuracy: 0.97 Validation accuracy: 0.86 Loss: 0.54745066\n",
      "95 Train accuracy: 0.97 Validation accuracy: 0.86 Loss: 0.53914857\n",
      "학습 종료\n",
      "INFO:tensorflow:Restoring parameters from models/11. 9_3_dnn_model\n",
      "최종 정확도: 0.82637316\n"
     ]
    }
   ],
   "source": [
    "#Run\n",
    "\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "\n",
    "max_epochs_without_progress = 50\n",
    "epochs_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "checkpoint_path = \"tmp/11. 9_3_dnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"models/11. 9_3_dnn_model\"\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, 'rb') as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"중지되었던 훈련입니다. 이어서 훈련합니다.\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        init.run()\n",
    "        \n",
    "    hidden_4_train = hidden4_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden_4_valid = hidden4_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_idices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h4_batch, y_batch = hidden_4_train[rnd_idices], y_train2[rnd_idices]\n",
    "            sess.run(training_op, feed_dict={hidden4_out: h4_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={hidden4_out: hidden_4_train, y: y_train2})\n",
    "        acc_val, loss_val, summary_str = sess.run([accuracy, loss, loss_summary], feed_dict={hidden4_out: hidden_4_valid, y: y_valid2})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Validation accuracy:\", acc_val, \"Loss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch+1))\n",
    "            if loss_val < best_loss:\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"학습 종료\")\n",
    "                    break\n",
    "    \n",
    "    save_path = saver.save(sess, final_model_path)\n",
    "    os.remove(checkpoint_epoch_path)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"최종 정확도:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"models/11. 8_dnn_model.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "hidden2_out = tf.get_default_graph().get_tensor_by_name(\"network/hidden2/Elu:0\")\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"network/logits/BiasAdd:0\")\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "unfrozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\"Adam3\")\n",
    "training_op = optimizer.minimize(loss, var_list=unfrozen_vars)\n",
    "\n",
    "loss_summary = tf.summary.scalar(\"log_loss\", loss)\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"CH11_9_4\"), tf.get_default_graph())\n",
    "init = tf.global_variables_initializer()\n",
    "two_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/11. 8_dnn_model\n",
      "a\n",
      "0 Train accuracy: 0.736 Validation accuracy: 0.70666665 Loss: 1.1232697\n",
      "5 Train accuracy: 0.982 Validation accuracy: 0.91333336 Loss: 0.3534832\n",
      "10 Train accuracy: 0.998 Validation accuracy: 0.91333336 Loss: 0.4194324\n",
      "15 Train accuracy: 1.0 Validation accuracy: 0.9 Loss: 0.44094884\n",
      "20 Train accuracy: 1.0 Validation accuracy: 0.9066667 Loss: 0.4705416\n",
      "25 Train accuracy: 1.0 Validation accuracy: 0.92 Loss: 0.48326153\n",
      "30 Train accuracy: 1.0 Validation accuracy: 0.9266667 Loss: 0.49529114\n",
      "35 Train accuracy: 1.0 Validation accuracy: 0.92 Loss: 0.50706255\n",
      "40 Train accuracy: 1.0 Validation accuracy: 0.92 Loss: 0.5204517\n",
      "45 Train accuracy: 1.0 Validation accuracy: 0.92 Loss: 0.53349787\n",
      "50 Train accuracy: 1.0 Validation accuracy: 0.92 Loss: 0.544478\n",
      "55 Train accuracy: 1.0 Validation accuracy: 0.92 Loss: 0.5544556\n",
      "60 Train accuracy: 1.0 Validation accuracy: 0.92 Loss: 0.5630434\n",
      "학습 종료\n",
      "INFO:tensorflow:Restoring parameters from models/11. 9_4_dnn_model\n",
      "최종 정확도: 0.8788315\n"
     ]
    }
   ],
   "source": [
    "#Run\n",
    "\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "\n",
    "max_epochs_without_progress = 50\n",
    "epochs_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "checkpoint_path = \"tmp/11. 9_4_dnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"models/11. 9_4_dnn_model\"\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, 'rb') as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"중지되었던 훈련입니다. 이어서 훈련합니다.\", start_epoch)\n",
    "        two_frozen_saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        init.run()\n",
    "    \n",
    "    restore_saver.restore(sess, \"models/11. 8_dnn_model\")\n",
    "    \n",
    "    hidden_2_train = hidden2_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden_2_valid = hidden2_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "    print('a')\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_idices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h2_batch, y_batch = hidden_2_train[rnd_idices], y_train2[rnd_idices]\n",
    "            sess.run(training_op, feed_dict={hidden2_out: h2_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={hidden2_out: hidden_2_train, y: y_train2})\n",
    "        acc_val, loss_val, summary_str = sess.run([accuracy, loss, loss_summary], feed_dict={hidden2_out: hidden_2_valid, y: y_valid2})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Validation accuracy:\", acc_val, \"Loss:\", loss_val)\n",
    "            two_frozen_saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch+1))\n",
    "            if loss_val < best_loss:\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"학습 종료\")\n",
    "                    break\n",
    "    \n",
    "    save_path = two_frozen_saver.save(sess, final_model_path)\n",
    "    os.remove(checkpoint_epoch_path)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    two_frozen_saver.restore(sess, final_model_path)\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"최종 정확도:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_outputs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2, n_inputs), name=\"X1\")\n",
    "X1, X2 = tf.unstack(X, axis=1)\n",
    "y = tf.placeholder(tf.int64, shape=[None, 1], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn_1\"):\n",
    "    hidden11 = hidden_layer_100(X1, name=\"hidden11\")\n",
    "    hidden12 = hidden_layer_100(hidden11, name=\"hidden12\")\n",
    "    hidden13 = hidden_layer_100(hidden12, name=\"hidden13\")\n",
    "    hidden14 = hidden_layer_100(hidden13, name=\"hidden14\")\n",
    "    hidden15 = hidden_layer_100(hidden14, name=\"hidden15\")\n",
    "\n",
    "with tf.name_scope(\"dnn_2\"):\n",
    "    hidden21 = hidden_layer_100(X2, name=\"hidden21\")\n",
    "    hidden22 = hidden_layer_100(hidden21, name=\"hidden22\")\n",
    "    hidden23 = hidden_layer_100(hidden22, name=\"hidden23\")\n",
    "    hidden24 = hidden_layer_100(hidden23, name=\"hidden24\")\n",
    "    hidden25 = hidden_layer_100(hidden24, name=\"hidden25\")\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    concat = tf.concat([hidden15, hidden25], axis=1)\n",
    "    hidden = tf.layers.dense(concat, 10, kernel_initializer=he_init, activation=tf.nn.elu)\n",
    "    logits = tf.layers.dense(hidden, 1, kernel_initializer=he_init, name=\"logits\")\n",
    "    y_proba = tf.nn.sigmoid(logits, name=\"y_proba\")\n",
    "    y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int64)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    y_as_float = tf.cast(y, tf.float32)\n",
    "    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar(\"log_loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"training\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.equal(y, y_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"CH11_10\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train\n",
    "y_train1 = y_train\n",
    "\n",
    "X_train2 = X_valid\n",
    "y_train2 = y_valid\n",
    "\n",
    "X_test = X_test\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(images, labels, batch_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < (batch_size // 2) :\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([1])\n",
    "        \n",
    "    while len(X) < batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if labels[rnd_idx1] != labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([0])\n",
    "    \n",
    "    rnd_idices = np.random.permutation(batch_size)\n",
    "    \n",
    "    return np.array(X)[rnd_idices], np.array(y)[rnd_idices]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2, 784)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "\n",
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 139.5, -0.5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3, 3 * batch_size))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_batch[:, 0].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_batch[:, 1].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1, y_test1 = generate_batch(X_test, y_test, batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 0.43221885 Accuracy: 0.7946\n",
      "1 Loss: 0.35233784 Accuracy: 0.8518\n",
      "2 Loss: 0.27449176 Accuracy: 0.8876\n",
      "3 Loss: 0.22187886 Accuracy: 0.9136\n",
      "4 Loss: 0.18677309 Accuracy: 0.9293\n",
      "5 Loss: 0.18210764 Accuracy: 0.9317\n",
      "6 Loss: 0.15667255 Accuracy: 0.9409\n",
      "7 Loss: 0.1297759 Accuracy: 0.9519\n",
      "8 Loss: 0.122631736 Accuracy: 0.9552\n",
      "9 Loss: 0.11801486 Accuracy: 0.9573\n",
      "10 Loss: 0.11044541 Accuracy: 0.9601\n",
      "11 Loss: 0.10215247 Accuracy: 0.9648\n",
      "12 Loss: 0.0953427 Accuracy: 0.9678\n",
      "13 Loss: 0.08792543 Accuracy: 0.9701\n",
      "14 Loss: 0.08516831 Accuracy: 0.9698\n",
      "15 Loss: 0.09070891 Accuracy: 0.972\n",
      "16 Loss: 0.08145917 Accuracy: 0.9726\n",
      "17 Loss: 0.090520196 Accuracy: 0.9702\n",
      "18 Loss: 0.0841903 Accuracy: 0.973\n",
      "19 Loss: 0.07462462 Accuracy: 0.977\n",
      "20 Loss: 0.07452233 Accuracy: 0.9775\n",
      "21 Loss: 0.079574615 Accuracy: 0.9747\n",
      "22 Loss: 0.087946475 Accuracy: 0.9728\n",
      "23 Loss: 0.07717004 Accuracy: 0.9769\n",
      "24 Loss: 0.06701691 Accuracy: 0.9797\n",
      "25 Loss: 0.07005286 Accuracy: 0.9796\n",
      "26 Loss: 0.07287023 Accuracy: 0.9765\n",
      "27 Loss: 0.06826186 Accuracy: 0.9782\n",
      "28 Loss: 0.072942495 Accuracy: 0.9788\n",
      "29 Loss: 0.06903705 Accuracy: 0.9791\n",
      "30 Loss: 0.075639665 Accuracy: 0.9785\n",
      "31 Loss: 0.0695895 Accuracy: 0.9795\n",
      "32 Loss: 0.07227024 Accuracy: 0.9783\n",
      "33 Loss: 0.073052086 Accuracy: 0.9801\n",
      "34 Loss: 0.07517022 Accuracy: 0.9793\n",
      "35 Loss: 0.088011935 Accuracy: 0.9762\n",
      "36 Loss: 0.0750419 Accuracy: 0.9799\n",
      "37 Loss: 0.08055899 Accuracy: 0.9793\n",
      "38 Loss: 0.075185396 Accuracy: 0.9803\n",
      "39 Loss: 0.071901426 Accuracy: 0.9801\n",
      "40 Loss: 0.06668567 Accuracy: 0.9811\n",
      "41 Loss: 0.073566005 Accuracy: 0.9807\n",
      "42 Loss: 0.069988266 Accuracy: 0.9797\n",
      "43 Loss: 0.072150424 Accuracy: 0.9807\n",
      "44 Loss: 0.06896522 Accuracy: 0.9813\n",
      "45 Loss: 0.069748595 Accuracy: 0.9803\n",
      "46 Loss: 0.06442966 Accuracy: 0.9816\n",
      "47 Loss: 0.068149686 Accuracy: 0.9817\n",
      "48 Loss: 0.07478245 Accuracy: 0.9801\n",
      "49 Loss: 0.07782133 Accuracy: 0.9797\n",
      "50 Loss: 0.075618595 Accuracy: 0.9798\n",
      "51 Loss: 0.07437096 Accuracy: 0.9804\n",
      "52 Loss: 0.071913116 Accuracy: 0.9804\n",
      "53 Loss: 0.081879355 Accuracy: 0.9792\n",
      "54 Loss: 0.06882492 Accuracy: 0.9812\n",
      "55 Loss: 0.06998796 Accuracy: 0.9825\n",
      "56 Loss: 0.07868076 Accuracy: 0.9807\n",
      "57 Loss: 0.07042049 Accuracy: 0.9818\n",
      "58 Loss: 0.08494332 Accuracy: 0.98\n",
      "59 Loss: 0.07222642 Accuracy: 0.9823\n",
      "60 Loss: 0.07857671 Accuracy: 0.9811\n",
      "61 Loss: 0.07319849 Accuracy: 0.9811\n",
      "62 Loss: 0.07037804 Accuracy: 0.9824\n",
      "63 Loss: 0.06905519 Accuracy: 0.9811\n",
      "64 Loss: 0.072664246 Accuracy: 0.9811\n",
      "65 Loss: 0.07545295 Accuracy: 0.9825\n",
      "66 Loss: 0.07851326 Accuracy: 0.9801\n",
      "67 Loss: 0.07449774 Accuracy: 0.9816\n",
      "68 Loss: 0.072989896 Accuracy: 0.9811\n",
      "69 Loss: 0.06701573 Accuracy: 0.983\n",
      "70 Loss: 0.078496926 Accuracy: 0.9826\n",
      "71 Loss: 0.08269567 Accuracy: 0.9814\n",
      "72 Loss: 0.07884539 Accuracy: 0.9822\n",
      "73 Loss: 0.06823293 Accuracy: 0.9822\n",
      "74 Loss: 0.07576585 Accuracy: 0.9837\n",
      "75 Loss: 0.078431986 Accuracy: 0.9828\n",
      "76 Loss: 0.07830181 Accuracy: 0.983\n",
      "77 Loss: 0.095104605 Accuracy: 0.9803\n",
      "78 Loss: 0.09358035 Accuracy: 0.9804\n",
      "79 Loss: 0.088144906 Accuracy: 0.9822\n",
      "80 Loss: 0.090188235 Accuracy: 0.9803\n",
      "81 Loss: 0.08482561 Accuracy: 0.9818\n",
      "82 Loss: 0.092876546 Accuracy: 0.9818\n",
      "83 Loss: 0.09613373 Accuracy: 0.9813\n",
      "84 Loss: 0.081555285 Accuracy: 0.9825\n",
      "85 Loss: 0.09351469 Accuracy: 0.9804\n",
      "Final Accuracy: 0.9804\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "best_loss = np.infty\n",
    "n_without_progress = 0\n",
    "max_n_without_progress = 10\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "final_model_path = \"models/11. 10_dnn_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_sum, loss_val, acc_val = sess.run([loss_summary, loss, accuracy], feed_dict={X: X_test1, y: y_test1})\n",
    "        print(epoch, \"Loss:\", loss_val, \"Accuracy:\", acc_val)\n",
    "        file_writer.add_summary(loss_sum, epoch)\n",
    "        \n",
    "        if epoch % 5 == 0 :\n",
    "            if loss_val < best_loss:\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                n_without_progress += 1\n",
    "                if n_without_progress > max_n_without_progress:\n",
    "                    break\n",
    "    \n",
    "    final_acc = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final Accuracy:\", final_acc)\n",
    "    \n",
    "    save_path = saver.save(sess, final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_outputs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=[None], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn_1\"):\n",
    "    hidden11 = hidden_layer_100(X, name=\"hidden11\")\n",
    "    hidden12 = hidden_layer_100(hidden11, name=\"hidden12\")\n",
    "    hidden13 = hidden_layer_100(hidden12, name=\"hidden13\")\n",
    "    hidden14 = hidden_layer_100(hidden13, name=\"hidden14\")\n",
    "    hidden15 = hidden_layer_100(hidden14, name=\"hidden15\")\n",
    "    frozen_hidden = tf.stop_gradient(hidden15)\n",
    "    \n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(frozen_hidden, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "    y_proba = tf.nn.sigmoid(logits, name=\"y_proba\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar(\"log_loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"training\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "dnn_1_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")\n",
    "restore_saver = tf.train.Saver(var_list = {var.op.name: var for var in dnn_1_vars})\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"CH11_10_2\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/11. 10_dnn_model\n",
      "0 Loss: 0.2841364 Accuracy: 0.9359\n",
      "1 Loss: 0.20358536 Accuracy: 0.9587\n",
      "2 Loss: 0.17913489 Accuracy: 0.965\n",
      "3 Loss: 0.16574754 Accuracy: 0.9692\n",
      "4 Loss: 0.15683977 Accuracy: 0.9704\n",
      "5 Loss: 0.15029614 Accuracy: 0.9714\n",
      "6 Loss: 0.14674853 Accuracy: 0.9712\n",
      "7 Loss: 0.1422701 Accuracy: 0.972\n",
      "8 Loss: 0.13990203 Accuracy: 0.9716\n",
      "9 Loss: 0.13753776 Accuracy: 0.9721\n",
      "10 Loss: 0.13507049 Accuracy: 0.9727\n",
      "11 Loss: 0.13245489 Accuracy: 0.9725\n",
      "12 Loss: 0.13325194 Accuracy: 0.9732\n",
      "13 Loss: 0.12932631 Accuracy: 0.9731\n",
      "14 Loss: 0.12838195 Accuracy: 0.9732\n",
      "15 Loss: 0.12718216 Accuracy: 0.9728\n",
      "16 Loss: 0.1274606 Accuracy: 0.9732\n",
      "17 Loss: 0.1242443 Accuracy: 0.9737\n",
      "18 Loss: 0.12507296 Accuracy: 0.9736\n",
      "19 Loss: 0.12359663 Accuracy: 0.9739\n",
      "20 Loss: 0.1220988 Accuracy: 0.9743\n",
      "21 Loss: 0.122726746 Accuracy: 0.9736\n",
      "22 Loss: 0.1218125 Accuracy: 0.9738\n",
      "23 Loss: 0.12021897 Accuracy: 0.9737\n",
      "24 Loss: 0.12066365 Accuracy: 0.9737\n",
      "25 Loss: 0.118903406 Accuracy: 0.9741\n",
      "26 Loss: 0.118576966 Accuracy: 0.9747\n",
      "27 Loss: 0.12005131 Accuracy: 0.975\n",
      "28 Loss: 0.11755193 Accuracy: 0.9743\n",
      "29 Loss: 0.12065926 Accuracy: 0.9745\n",
      "30 Loss: 0.12200947 Accuracy: 0.9731\n",
      "31 Loss: 0.1189156 Accuracy: 0.9739\n",
      "32 Loss: 0.11939528 Accuracy: 0.9743\n",
      "33 Loss: 0.11688418 Accuracy: 0.9745\n",
      "34 Loss: 0.117211744 Accuracy: 0.9742\n",
      "35 Loss: 0.116646655 Accuracy: 0.9744\n",
      "36 Loss: 0.118752345 Accuracy: 0.9746\n",
      "37 Loss: 0.118233584 Accuracy: 0.9751\n",
      "38 Loss: 0.11798555 Accuracy: 0.9745\n",
      "39 Loss: 0.11893171 Accuracy: 0.9755\n",
      "40 Loss: 0.116959274 Accuracy: 0.9747\n",
      "41 Loss: 0.11892178 Accuracy: 0.9741\n",
      "42 Loss: 0.12117374 Accuracy: 0.9745\n",
      "43 Loss: 0.11896957 Accuracy: 0.9748\n",
      "44 Loss: 0.11845055 Accuracy: 0.9741\n",
      "45 Loss: 0.119643435 Accuracy: 0.9742\n",
      "46 Loss: 0.120521694 Accuracy: 0.9747\n",
      "47 Loss: 0.116837844 Accuracy: 0.9744\n",
      "48 Loss: 0.120311595 Accuracy: 0.9747\n",
      "49 Loss: 0.12145981 Accuracy: 0.9748\n",
      "50 Loss: 0.12070702 Accuracy: 0.9746\n",
      "51 Loss: 0.120338805 Accuracy: 0.9747\n",
      "52 Loss: 0.121178344 Accuracy: 0.9745\n",
      "53 Loss: 0.12081362 Accuracy: 0.9755\n",
      "54 Loss: 0.122262135 Accuracy: 0.9751\n",
      "55 Loss: 0.123239234 Accuracy: 0.9742\n",
      "56 Loss: 0.12416035 Accuracy: 0.9735\n",
      "57 Loss: 0.123374 Accuracy: 0.9735\n",
      "58 Loss: 0.12404132 Accuracy: 0.9744\n",
      "59 Loss: 0.12988122 Accuracy: 0.9729\n",
      "60 Loss: 0.12623054 Accuracy: 0.9738\n",
      "61 Loss: 0.12575276 Accuracy: 0.974\n",
      "62 Loss: 0.12506998 Accuracy: 0.9741\n",
      "63 Loss: 0.12797962 Accuracy: 0.9737\n",
      "64 Loss: 0.12890881 Accuracy: 0.9736\n",
      "65 Loss: 0.1272884 Accuracy: 0.9742\n",
      "66 Loss: 0.12738061 Accuracy: 0.9747\n",
      "67 Loss: 0.13215615 Accuracy: 0.9735\n",
      "68 Loss: 0.12905805 Accuracy: 0.9754\n",
      "69 Loss: 0.13000998 Accuracy: 0.9732\n",
      "70 Loss: 0.1315994 Accuracy: 0.9734\n",
      "71 Loss: 0.13126796 Accuracy: 0.9743\n",
      "72 Loss: 0.13047983 Accuracy: 0.9753\n",
      "73 Loss: 0.13172707 Accuracy: 0.9753\n",
      "74 Loss: 0.13235267 Accuracy: 0.9736\n",
      "75 Loss: 0.1339746 Accuracy: 0.9744\n",
      "76 Loss: 0.13654822 Accuracy: 0.9735\n",
      "77 Loss: 0.13496682 Accuracy: 0.9735\n",
      "78 Loss: 0.13386331 Accuracy: 0.9748\n",
      "79 Loss: 0.13967493 Accuracy: 0.9732\n",
      "80 Loss: 0.13448502 Accuracy: 0.9745\n",
      "81 Loss: 0.13504438 Accuracy: 0.9741\n",
      "82 Loss: 0.13883635 Accuracy: 0.9744\n",
      "83 Loss: 0.13753788 Accuracy: 0.9735\n",
      "84 Loss: 0.14164853 Accuracy: 0.9728\n",
      "85 Loss: 0.13735174 Accuracy: 0.9739\n",
      "Final Accuracy: 0.9739\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "best_loss = np.infty\n",
    "n_without_progress = 0\n",
    "max_n_without_progress = 10\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "restore_model_path = \"models/11. 10_dnn_model\"\n",
    "final_model_path = \"models/11. 10_2_dnn_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, restore_model_path)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_idices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_idices], y_train2[rnd_idices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_sum, loss_val, acc_val = sess.run([loss_summary, loss, accuracy], feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Loss:\", loss_val, \"Accuracy:\", acc_val)\n",
    "        file_writer.add_summary(loss_sum, epoch)\n",
    "        \n",
    "        if epoch % 5 == 0 :\n",
    "            if loss_val < best_loss:\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                n_without_progress += 1\n",
    "                if n_without_progress > max_n_without_progress:\n",
    "                    break\n",
    "    \n",
    "    final_acc = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "    print(\"Final Accuracy:\", final_acc)\n",
    "    \n",
    "    save_path = saver.save(sess, final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not_restored\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_outputs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=[None], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn_1\"):\n",
    "    hidden11 = hidden_layer_100(X, name=\"hidden11\")\n",
    "    hidden12 = hidden_layer_100(hidden11, name=\"hidden12\")\n",
    "    hidden13 = hidden_layer_100(hidden12, name=\"hidden13\")\n",
    "    hidden14 = hidden_layer_100(hidden13, name=\"hidden14\")\n",
    "    hidden15 = hidden_layer_100(hidden14, name=\"hidden15\")\n",
    "    \n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(hidden15, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "    y_proba = tf.nn.sigmoid(logits, name=\"y_proba\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar(\"log_loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"training\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter(log_dir(\"CH11_10_3\"), tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 0.3412308 Accuracy: 0.8941\n",
      "1 Loss: 0.30026144 Accuracy: 0.9064\n",
      "2 Loss: 0.25832838 Accuracy: 0.9189\n",
      "3 Loss: 0.25780922 Accuracy: 0.9213\n",
      "4 Loss: 0.23832795 Accuracy: 0.9274\n",
      "5 Loss: 0.23137446 Accuracy: 0.9359\n",
      "6 Loss: 0.26874304 Accuracy: 0.9302\n",
      "7 Loss: 0.2931884 Accuracy: 0.9256\n",
      "8 Loss: 0.32790685 Accuracy: 0.9207\n",
      "9 Loss: 0.40393725 Accuracy: 0.9096\n",
      "10 Loss: 0.31849623 Accuracy: 0.9302\n",
      "11 Loss: 0.30762684 Accuracy: 0.9299\n",
      "12 Loss: 0.29522055 Accuracy: 0.936\n",
      "13 Loss: 0.26068005 Accuracy: 0.9442\n",
      "14 Loss: 0.2541512 Accuracy: 0.9451\n",
      "15 Loss: 0.25289693 Accuracy: 0.9454\n",
      "16 Loss: 0.25485116 Accuracy: 0.9458\n",
      "17 Loss: 0.25479415 Accuracy: 0.946\n",
      "18 Loss: 0.25684464 Accuracy: 0.9458\n",
      "19 Loss: 0.2577395 Accuracy: 0.9466\n",
      "20 Loss: 0.25997987 Accuracy: 0.9466\n",
      "21 Loss: 0.2604073 Accuracy: 0.9467\n",
      "22 Loss: 0.26226738 Accuracy: 0.947\n",
      "23 Loss: 0.26426473 Accuracy: 0.9465\n",
      "24 Loss: 0.26465216 Accuracy: 0.9469\n",
      "25 Loss: 0.26678178 Accuracy: 0.9468\n",
      "26 Loss: 0.26834008 Accuracy: 0.9474\n",
      "27 Loss: 0.26908422 Accuracy: 0.9475\n",
      "28 Loss: 0.2709517 Accuracy: 0.9472\n",
      "29 Loss: 0.27145258 Accuracy: 0.947\n",
      "30 Loss: 0.27326456 Accuracy: 0.9472\n",
      "31 Loss: 0.27469668 Accuracy: 0.9472\n",
      "32 Loss: 0.27628535 Accuracy: 0.947\n",
      "33 Loss: 0.27775878 Accuracy: 0.9471\n",
      "34 Loss: 0.2795716 Accuracy: 0.9473\n",
      "35 Loss: 0.28100255 Accuracy: 0.947\n",
      "36 Loss: 0.28201258 Accuracy: 0.9469\n",
      "37 Loss: 0.28363314 Accuracy: 0.9472\n",
      "38 Loss: 0.2849833 Accuracy: 0.947\n",
      "39 Loss: 0.28631523 Accuracy: 0.947\n",
      "40 Loss: 0.28769627 Accuracy: 0.9467\n",
      "41 Loss: 0.28924575 Accuracy: 0.9468\n",
      "42 Loss: 0.29042304 Accuracy: 0.9471\n",
      "43 Loss: 0.29219404 Accuracy: 0.9468\n",
      "44 Loss: 0.29363456 Accuracy: 0.9468\n",
      "45 Loss: 0.2945539 Accuracy: 0.9472\n",
      "46 Loss: 0.29573306 Accuracy: 0.9475\n",
      "47 Loss: 0.29767096 Accuracy: 0.9474\n",
      "48 Loss: 0.29890212 Accuracy: 0.9471\n",
      "49 Loss: 0.30036733 Accuracy: 0.9473\n",
      "50 Loss: 0.30193073 Accuracy: 0.9471\n",
      "51 Loss: 0.30288038 Accuracy: 0.9474\n",
      "52 Loss: 0.3049719 Accuracy: 0.9469\n",
      "53 Loss: 0.30633575 Accuracy: 0.9471\n",
      "54 Loss: 0.30695182 Accuracy: 0.9474\n",
      "55 Loss: 0.30857408 Accuracy: 0.9474\n",
      "56 Loss: 0.31082496 Accuracy: 0.9471\n",
      "57 Loss: 0.3115967 Accuracy: 0.9472\n",
      "58 Loss: 0.3135156 Accuracy: 0.9467\n",
      "59 Loss: 0.31497294 Accuracy: 0.9474\n",
      "60 Loss: 0.31644562 Accuracy: 0.9472\n",
      "Final Accuracy: 0.9472\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10001\n",
    "best_loss = np.infty\n",
    "n_without_progress = 0\n",
    "max_n_without_progress = 10\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "final_model_path = \"models/11. 10_3_dnn_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_idices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_idices], y_train2[rnd_idices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_sum, loss_val, acc_val = sess.run([loss_summary, loss, accuracy], feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, \"Loss:\", loss_val, \"Accuracy:\", acc_val)\n",
    "        file_writer.add_summary(loss_sum, epoch)\n",
    "        \n",
    "        if epoch % 5 == 0 :\n",
    "            if loss_val < best_loss:\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                n_without_progress += 1\n",
    "                if n_without_progress > max_n_without_progress:\n",
    "                    break\n",
    "    \n",
    "    final_acc = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "    print(\"Final Accuracy:\", final_acc)\n",
    "    \n",
    "    save_path = saver.save(sess, final_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
